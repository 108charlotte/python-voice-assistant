<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>JARVIS</title>

    <link href="https://fonts.googleapis.com/css2?family=Rubik:wght@400;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
    <link rel="icon" type="image/png" href="{{ url_for('static', filename='favicon.png') }}">
</head>
</head>
<body>
    <h2>JARVIS Voice Assistant</h2>
    
    <button id="record">Bother me</button>
    <button id="stop">Shut Up</button>
    <br><br>
    <div id="status"></div>
    <audio id="voice" controls></audio>
    <canvas id="visualizer" width="400" height="200"></canvas>

    <script>
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const recordBtn = document.getElementById('record');
        const stopBtn = document.getElementById('stop');
        const statusDiv = document.getElementById('status');
        const audio = document.getElementById("voice");
        const canvas = document.getElementById("visualizer");
        const ctx = canvas.getContext("2d");
        let recognition, listening = false;

        const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        const analyser = audioCtx.createAnalyser();
        analyser.fftSize = 64;
        const bufferLength = analyser.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);

        const source = audioCtx.createMediaElementSource(audio);
        source.connect(analyser);
        analyser.connect(audioCtx.destination);

        recordBtn.onclick = () => {
            if (!SpeechRecognition) {
                alert("Web Speech API not supported.");
                return;
            }
            recognition = new SpeechRecognition();
            recognition.lang = 'en-US';
            recognition.interimResults = false;
            recognition.continuous = false;
            listening = true;
            recordBtn.disabled = true;
            stopBtn.disabled = false;

            recognition.onstart = () => {
                console.log("Recognition started");
            };

            recognition.onresult = async (event) => {
                console.log("Recognition result:", event);
                const transcript = event.results[0][0].transcript;
                statusDiv.innerHTML = `You said: "${transcript}"<br><br>Processing... <img src="https://i.imgur.com/llF5iyg.gif" alt="Loading" style="width:24px;height:24px;vertical-align:middle;">`;
                const response = await fetch('/upload', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ transcript })
                });
                const result = await response.json();
                audio.src = result.audio_url + '?v=' + Date.now();

                setTimeout(() => {
                    audio.play();
                }, 1000);
            };
            recognition.onend = () => {
                console.log("Recognition ended");
                if (listening) setTimeout(() => recognition.start(), 500);
            };
            recognition.onerror = (e) => {
                console.log("Recognition error:", e);
                if (e.error === "no-speech" && listening) {
                    setTimeout(() => recognition.start(), 500);
                } else {
                    listening = false;
                    recordBtn.disabled = false;
                }
            };
            recognition.start();
        };

        stopBtn.onclick = () => {
            listening = false;
            recordBtn.disabled = false;
            stopBtn.disabled = true;
            if (recognition) recognition.stop();
        };
        
        function animate(){
            x = 0; 
            ctx.clearRect(0, 0, canvas.width, canvas.height); 
            analyser.getByteFrequencyData(dataArray); 
            drawVisualizer(bufferLength, x, barWidth, barHeight, dataArray); 
            requestAnimationFrame(animate); 
        }

        audio.onplay = () => {
            // Resume the audio context if needed (required by some browsers)
            if (audioCtx.state === 'suspended') {
                audioCtx.resume();
            }
            drawVisualizer();
        };

        animate(); 

        function drawVisualizer(bufferLength, x, barWidth, barHeight, dataArray) {
            for (let i = 0; i < bufferLength; i++) {
                barHeight = dataArray[i] * 0.8; 
                ctx.save(); 
                ctx.translate(canvas.width/2, canvas.height/2); 
                ctx.rotate(i * 0.14); 
                const hue = i * 1.5; 
                ctx.fillStyle = 'hsl()' + hue + ', 100%,' + barHeight/3 + '%'; 
                ctx.fillRect(0, 0, barWidth, barHeight); 
                x += barWidth; 
                ctx.restore(); 
            }
        }

        function drawVisualizer() {
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            analyser.getByteFrequencyData(dataArray);
            let barWidth = canvas.width / bufferLength;
            let x = 0;
            for (let i = 0; i < bufferLength; i++) {
                let barHeight = dataArray[i] * 0.8;
                ctx.fillStyle = `hsl(${i * 10}, 100%, 50%)`;
                ctx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);
                x += barWidth;
            }
            requestAnimationFrame(drawVisualizer);
        }
    </script>
</body>
</html>