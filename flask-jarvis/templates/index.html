<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>JARVIS</title>

    <link href="https://fonts.googleapis.com/css2?family=Rubik:wght@400;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
    <link rel="icon" type="image/png" href="{{ url_for('static', filename='favicon.png') }}">
</head>
</head>
<body>
    <h2>JARVIS Voice Assistant</h2>
    
    <button id="record">Bother me</button>
    <button id="stop">Shut Up</button>
    <br><br>
    <div id="status"></div>
    <audio id="voice" controls></audio>
    <canvas id="audio_visualizer" width="400" height="200"></canvas>

    <script>
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const recordBtn = document.getElementById('record');
        const stopBtn = document.getElementById('stop');
        const statusDiv = document.getElementById('status');
        const audio = document.getElementById("voice");
        const canvas = document.getElementById("audio_visualizer");
        const ctx = canvas.getContext("2d");
        let recognition, listening = false;

        recordBtn.onclick = () => {
            if (!SpeechRecognition) {
                alert("Web Speech API not supported.");
                return;
            }
            recognition = new SpeechRecognition();
            recognition.lang = 'en-US';
            recognition.interimResults = false;
            recognition.continuous = false;
            listening = true;
            recordBtn.disabled = true;
            stopBtn.disabled = false;

            recognition.onstart = () => {
                console.log("Recognition started");
            };

            recognition.onresult = async (event) => {
                console.log("Recognition result:", event);
                const transcript = event.results[0][0].transcript;
                statusDiv.innerHTML = `You said: "${transcript}"<br><br>Processing... <img src="https://i.imgur.com/llF5iyg.gif" alt="Loading" style="width:24px;height:24px;vertical-align:middle;">`;
                const response = await fetch('/upload', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ transcript })
                });
                const result = await response.json();
                audio.src = result.audio_url + '?v=' + Date.now();

                setTimeout(() => {
                    audio.play();
                }, 1000);
            };
            recognition.onend = () => {
                console.log("Recognition ended");
                if (listening) setTimeout(() => recognition.start(), 500);
            };
            recognition.onerror = (e) => {
                console.log("Recognition error:", e);
                if (e.error === "no-speech" && listening) {
                    setTimeout(() => recognition.start(), 500);
                } else {
                    listening = false;
                    recordBtn.disabled = false;
                }
            };
            recognition.start();
        };

        stopBtn.onclick = () => {
            listening = false;
            recordBtn.disabled = false;
            stopBtn.disabled = true;
            if (recognition) recognition.stop();
        };

        class Bar {
            constructor(centerX, centerY, radius, angle, color, index) {
                this.centerX = centerX;
                this.centerY = centerY;
                this.radius = radius;
                this.angle = angle;
                this.length = 0;
                this.color = color;
                this.index = index;
            }
            update(micInput) {
                this.length = (micInput / 255) * 60 + 10;
            }
            draw(context) {
                const x1 = this.centerX + Math.cos(this.angle) * this.radius;
                const y1 = this.centerY + Math.sin(this.angle) * this.radius;
                const x2 = this.centerX + Math.cos(this.angle) * (this.radius + this.length);
                const y2 = this.centerY + Math.sin(this.angle) * (this.radius + this.length);
                context.strokeStyle = this.color;
                context.lineWidth = 4;
                context.beginPath();
                context.moveTo(x1, y1);
                context.lineTo(x2, y2);
                context.stroke();
            }
        }

        const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        const analyser = audioCtx.createAnalyser();
        analyser.fftSize = 64; // You can adjust for more/less bars
        const bufferLength = analyser.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);

        const source = audioCtx.createMediaElementSource(audio);
        source.connect(analyser);
        analyser.connect(audioCtx.destination);

        // Create an array to hold your Bar objects
        const bars = [];
        const centerX = canvas.width / 2;
        const centerY = canvas.height / 2;
        const radius = 40;
        for (let i = 0; i < bufferLength; i++) {
            const angle = (i / bufferLength) * Math.PI * 2;
            const color = `hsl(${200 + (i * 2) % 60}, 100%, 60%)`; // blue/cyan spiral
            bars.push(new Bar(centerX, centerY, radius, angle, color, i));
        }

        function animate() {
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            analyser.getByteFrequencyData(dataArray);

            for (let i = 0; i < bufferLength; i++) {
                bars[i].update(dataArray[i]);
                bars[i].draw(ctx);
            }
            requestAnimationFrame(animate);
        }
        audio.onplay = () => {
            if (audioCtx.state === 'suspended') audioCtx.resume();
            animate();
        };
        animate(); 
    </script>
</body>
</html>