<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>JARVIS</title>

    <link href="https://fonts.googleapis.com/css2?family=Rubik:wght@400;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
    <link rel="icon" type="image/png" href="{{ url_for('static', filename='favicon.png') }}">
</head>
<body>
    <h2>JARVIS Voice Assistant</h2>
    <div id="jarvis-container">
        <div id="button-row">
            <button id="record">Bother me</button>
            <button id="stop">Shut Up</button>
        </div>
        <div id="status"><div style="color:#888;">Click Bother me to start</div></div>
        <audio id="voice" controls></audio>
        <canvas id="audio_visualizer" width="400" height="200"></canvas>
        <div id="todo-container">
            <h3>Todo List</h3>
            <ul id="todo-list"></ul>
            <div id="todo-input-row">
                <input id="new-task" type="text" placeholder="Add a task...">
                <button id="add-task-btn">Add</button>
            </div>
        </div>
    </div>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            let convohistory = {
                history: [],
                in_focus_mode: false, 
                tasks: []
            };
            
            let recognizing = false; 
            let listening = false;
            let hasUserSpoken = false; 
            let lastUserPrompt = "";
            let todoList = []
            let currentController = null;
            let lastRequestId = 0;

            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            const recordBtn = document.getElementById('record');
            const stopBtn = document.getElementById('stop');
            const statusDiv = document.getElementById('status');
            const audio = document.getElementById("voice");
            const canvas = document.getElementById("audio_visualizer");
            const ctx = canvas.getContext("2d");
            let recognition;

            if (SpeechRecognition) {
                recognition = new SpeechRecognition();
                recognition.lang = 'en-US';
                recognition.interimResults = false;
                recognition.continuous = false;

                recognition.onstart = () => {
                    recognizing = true;
                    console.log("%cRecognition started, recognizing:", "color: gray", recognizing, "listening:", listening);
                    let focusStatus = convohistory.in_focus_mode
                        ? `<div class="status-line focus-status">Focus Mode: ON</div>`
                        : "";

                    statusDiv.innerHTML =
                        focusStatus +
                        `<div class="status-line listening-status">Listening...</div>` +
                        `<div class="status-line you-said-status">${lastUserPrompt ? `You said: "${lastUserPrompt}"` : `<div class="not-listening-status">Say something to interact with JARVIS</div>`}</div>`;
                };

                recognition.onresult = async (event) => {
                    const transcript = event.results[0][0].transcript;
                    lastUserPrompt = transcript;
                    const lastAssistant = convohistory.history.length >= 2
                        ? convohistory.history[convohistory.history.length - 2]
                        : null;

                    function isFocusModeCommand(text) {
                        const t = text.toLowerCase();
                        return (
                            (t.includes("focus") || t.includes("focusing")) &&
                            (
                                ["activate", "begin", "start", "deactivate", "end", "stop"].some(word => t.includes(word))
                            )
                        );
                    }

                    if (
                        lastAssistant && lastAssistant.content &&
                        lastAssistant.content.trim().toLowerCase() === transcript.trim().toLowerCase() &&
                        !isFocusModeCommand(transcript)
                    ) {
                        console.log("[DEBUG] Transcript exactly matches last assistant message, skipping backend call.");
                        return;
                    }

                    if (
                        (!convohistory.history.length ||
                            convohistory.history[convohistory.history.length - 1].role !== "user" ||
                            convohistory.history[convohistory.history.length - 1].content !== transcript) &&
                        (!lastAssistant || lastAssistant.content !== transcript)
                    ) {
                        convohistory.history.push({ role: "user", content: transcript });
                    }

                    statusDiv.innerHTML =
                        `<div class="status-line processing-status">Processing... <img src="https://i.imgur.com/llF5iyg.gif" style="width:24px;height:24px;vertical-align:middle;"></div>` +
                        `<div style="margin-top:8px;">You said: "${transcript}"</div>`;

                    if (currentController) {
                        currentController.abort();
                    }
                    currentController = new AbortController();

                    try {
                        audio.pause(); 
                        audio.currentTime = 0; 
                        const requestId = ++lastRequestId;
                        const response = await fetch('/upload', {
                            method: 'POST',
                            headers: { 'Content-Type': 'application/json' },
                            body: JSON.stringify({ transcript, convohistory }),
                            signal: currentController.signal
                        });
                        const result = await response.json();
                        convohistory = result.convohistory || convohistory;
                        updateTodoVisibility();

                        if (requestId === lastRequestId && result.audio_url) {
                            audio.pause();
                            audio.currentTime = 0;
                            audio.src = result.audio_url + '?v=' + Date.now();
                            setTimeout(() => {
                                audio.play().catch(e => {
                                    console.log("Audio play error:", e);
                                    audio.onended();
                                });
                            }, 1000);
                        } else {
                            audio.onended();
                        }

                        let focusStatus = convohistory.in_focus_mode
                            ? `<div class="status-line focus-status">Focus Mode: ON</div>`
                            : "";

                        statusDiv.innerHTML =
                            focusStatus +
                            `<div class="status-line not-listening-status">Not listening.</div>` +
                            `<div class="status-line you-said-status">You said: "${lastUserPrompt}"</div>`;
                    } catch (err) {
                        if (err.name === 'AbortError') {
                            console.log('Previous request aborted');
                        } else {
                            console.error("Error during fetch or JSON parsing:", err);
                        }
                    }
                };

                recognition.onend = () => {
                    recognizing = false;
                    console.log("%cRecognition ended, recognizing:", "color: gray", recognizing, "listening:", listening);

                    let focusStatus = convohistory.in_focus_mode
                        ? `<div class="status-line focus-status">Focus Mode: ON</div>`
                        : "";

                    statusDiv.innerHTML =
                        focusStatus +
                        `<div class="status-line not-listening-status">Not listening.</div>` +
                        `<div class="status-line you-said-status">${lastUserPrompt ? `You said: "${lastUserPrompt}"` : ""}</div>`;

                    // If still listening and recognition is not running, and audio is not playing, restart recognition
                    if (listening && !recognizing && (audio.paused || audio.ended)) {
                        setTimeout(() => {
                            try {
                                recognition.start();
                            } catch (e) {
                                console.log("Recognition start error:", e);
                            }
                        }, 200); // Try 200ms instead of 1000ms
                    }
                };

                recognition.onerror = (e) => {
                    console.log("%cRecognition error:", "color: gray", e);
                    if (listening) {
                        setTimeout(() => {
                            try {
                                recognition.start();
                            } catch (err) {
                                console.log("Recognition start error:", err);
                            }
                        }, 1000);
                    } else {
                        recordBtn.disabled = false;
                        stopBtn.disabled = true;
                    }
                };
            } else {
                alert("Web Speech API not supported.");
            }

            recordBtn.onclick = () => {
                if (!SpeechRecognition) {
                    alert("Web Speech API not supported.");
                    return;
                }
                if (!recognizing) {
                    listening = true;
                    recognition.start();
                    recordBtn.disabled = true;
                    stopBtn.disabled = false;
                    
                }
            };

            stopBtn.onclick = () => {
                listening = false;
                recordBtn.disabled = false;
                stopBtn.disabled = true;
                if (recognition) recognition.stop();
                audio.pause();
                audio.currentTime = 0; 
                statusDiv.innerHTML = ""; 
            };

            class Bar {
                constructor(centerX, centerY, radius, angle, color, index) {
                    this.centerX = centerX;
                    this.centerY = centerY;
                    this.radius = radius;
                    this.angle = angle;
                    this.length = 0;
                    this.color = color;
                    this.index = index;
                }
                update(micInput) {
                    this.length = (micInput / 255) * 60 + 10;
                }
                draw(context) {
                    const x1 = this.centerX + Math.cos(this.angle) * this.radius;
                    const y1 = this.centerY + Math.sin(this.angle) * this.radius;
                    const x2 = this.centerX + Math.cos(this.angle) * (this.radius + this.length);
                    const y2 = this.centerY + Math.sin(this.angle) * (this.radius + this.length);
                    context.strokeStyle = this.color;
                    context.lineWidth = 4;
                    context.beginPath();
                    context.moveTo(x1, y1);
                    context.lineTo(x2, y2);
                    context.stroke();
                }
            }

            const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
            const analyser = audioCtx.createAnalyser();
            analyser.fftSize = 64; // You can adjust for more/less bars
            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);

            const source = audioCtx.createMediaElementSource(audio);
            source.connect(analyser);
            analyser.connect(audioCtx.destination);

            // Create an array to hold your Bar objects
            const bars = [];
            const centerX = canvas.width / 2;
            const centerY = canvas.height / 2;
            const radius = 40;
            for (let i = 0; i < bufferLength; i++) {
                const angle = (i / bufferLength) * Math.PI * 2;
                const color = `hsl(${200 + (i * 2) % 60}, 100%, 60%)`; // blue/cyan spiral
                bars.push(new Bar(centerX, centerY, radius, angle, color, i));
            }

            function animate() {
                ctx.clearRect(0, 0, canvas.width, canvas.height);
                analyser.getByteFrequencyData(dataArray);

                for (let i = 0; i < bufferLength; i++) {
                    bars[i].update(dataArray[i]);
                    bars[i].draw(ctx);
                }
                requestAnimationFrame(animate);
            }

            audio.onplay = () => {
                if (audioCtx.state === 'suspended') audioCtx.resume();
                animate();
            };

            audio.onended = () => {
                console.log("Audio ended, recognizing:", recognizing, "listening:", listening);
                if (listening && !recognizing) {
                    setTimeout(() => {
                        try {
                            recognition.start();
                        } catch (e) {
                            console.log("Recognition start error:", e);
                        }
                    }, 200); // Try 200ms instead of 1000ms
                }
            };

            animate(); 

            function updateTodoVisibility() {
                const todoContainer = document.getElementById('todo-container');
                if (convohistory.in_focus_mode) {
                    todoContainer.style.display = "flex";
                    renderTodoList(); 
                } else {
                    todoContainer.style.display = "none";
                }
            }

            function renderTodoList() {
                const todoListEl = document.getElementById('todo-list');
                todoListEl.innerHTML = "";
                if ((convohistory.tasks || []).length === 0) {
                    const emptyMsg = document.createElement('li');
                    emptyMsg.textContent = "No tasks yet. ";
                    emptyMsg.style.color = "#aaa";
                    todoListEl.appendChild(emptyMsg);
                }
                else {
                    (convohistory.tasks || []).forEach((task, idx) => {
                        const li = document.createElement('li');
                        li.textContent = task;
                        
                        const removeBtn = document.createElement('button'); 
                        removeBtn.textContent = "🗑️"; 
                        removeBtn.title = "Remove task"; 

                        removeBtn.onclick = (e) => {
                            e.stopPropagation();
                            convohistory.tasks.splice(idx, 1);
                            renderTodoList();
                        };

                        li.appendChild(removeBtn);
                        todoListEl.appendChild(li);
                    });
                }
            }

            document.getElementById('new-task').addEventListener('keydown', function(e) {
                if (e.key === "Enter") {
                    document.getElementById('add-task-btn').click();
                }
            });

            document.getElementById('add-task-btn').onclick = () => {
                    const input = document.getElementById('new-task');
                    const task = input.value.trim();
                    if (task) {
                        convohistory.tasks = convohistory.tasks || [];
                        convohistory.tasks.push(task);
                        input.value = "";
                        renderTodoList();
                    }
                };
        });
    </script>
</body>
</html>