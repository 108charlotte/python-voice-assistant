<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>JARVIS</title>

    <link href="https://fonts.googleapis.com/css2?family=Rubik:wght@400;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
    <link rel="icon" type="image/png" href="{{ url_for('static', filename='favicon.png') }}">
</head>
</head>
<body>
    <h2>JARVIS Voice Assistant</h2>
    <div id="jarvis-container">
        <div id="button-row">
            <button id="record">Bother me</button>
            <button id="stop">Shut Up</button>
        </div>
        <div id="status"><div style="color:#888;">Click Bother me to start</div></div>
        <audio id="voice" controls></audio>
        <canvas id="audio_visualizer" width="400" height="200"></canvas>
    </div>
    <script>
        let convohistory = [];
        let recognizing = false; 
        let listening = false;
        let hasUserSpoken = false; 
        let lastUserPrompt = "";

        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const recordBtn = document.getElementById('record');
        const stopBtn = document.getElementById('stop');
        const statusDiv = document.getElementById('status');
        const audio = document.getElementById("voice");
        const canvas = document.getElementById("audio_visualizer");
        const ctx = canvas.getContext("2d");
        let recognition;

        if (SpeechRecognition) {
            recognition = new SpeechRecognition();
            recognition.lang = 'en-US';
            recognition.interimResults = false;
            recognition.continuous = false;

            recognition.onstart = () => {
                recognizing = true;
                statusDiv.innerHTML =
                    `<div style="color:#00bcd4;">Listening...</div>` +
                    (lastUserPrompt ? `<div style="margin-top:8px;">You said: "${lastUserPrompt}"</div>` : "");
                console.log("Recognition started");
            };

            recognition.onresult = async (event) => {
                const transcript = event.results[0][0].transcript;
                lastUserPrompt = transcript;
                statusDiv.innerHTML =
                    `<div style="color:#4caf50;">Processing... <img src="https://i.imgur.com/llF5iyg.gif" alt="Loading" style="width:24px;height:24px;vertical-align:middle;"></div>` +
                    `<div style="margin-top:8px;">You said: "${transcript}"</div>`;
                const response = await fetch('/upload', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ transcript, convohistory })
                });
                const result = await response.json();
                convohistory = result.convohistory || [];
                audio.src = result.audio_url + '?v=' + Date.now();

                statusDiv.innerHTML =
                    `<div style="color:#00bcd4;">Listening...</div>` +
                    `<div style="margin-top:8px;">You said: "${transcript}"</div>`;

                setTimeout(() => {
                    audio.play();
                }, 1000);
            };

            recognition.onend = () => {
                recognizing = false; 
                console.log("Recognition ended");
                if (listening) {
                    setTimeout(() => {
                        if (!recognizing) recognition.start();
                    }, 100);
                }
                // Always update status
                if (!listening) {
                    statusDiv.innerHTML = `<div style="color:#888;">Stopped listening.</div>`;
                }
            };

            recognition.onerror = (e) => {
                console.log("Recognition error:", e);
                if (e.error === "no-speech" && listening) {
                    setTimeout(() => recognition.start(), 500);
                } else if (e.error === "network" && listening) {
                    // Try to restart after a short delay
                    setTimeout(() => recognition.start(), 1500);
                } else {
                    listening = false;
                    recordBtn.disabled = false;
                    stopBtn.disabled = true;
                }
            };
        } else {
            alert("Web Speech API not supported.");
        }

        recordBtn.onclick = () => {
            if (!SpeechRecognition) {
                alert("Web Speech API not supported.");
                return;
            }
            if (!recognizing) {
                listening = true;
                recognition.start();
                recordBtn.disabled = true;
                stopBtn.disabled = false;
                
            }
        };

        stopBtn.onclick = () => {
            listening = false; // Set this first!
            recordBtn.disabled = false;
            stopBtn.disabled = true;
            if (recognition) recognition.stop();
            statusDiv.innerHTML = ""; // Optionally clear status
        };

        class Bar {
            constructor(centerX, centerY, radius, angle, color, index) {
                this.centerX = centerX;
                this.centerY = centerY;
                this.radius = radius;
                this.angle = angle;
                this.length = 0;
                this.color = color;
                this.index = index;
            }
            update(micInput) {
                this.length = (micInput / 255) * 60 + 10;
            }
            draw(context) {
                const x1 = this.centerX + Math.cos(this.angle) * this.radius;
                const y1 = this.centerY + Math.sin(this.angle) * this.radius;
                const x2 = this.centerX + Math.cos(this.angle) * (this.radius + this.length);
                const y2 = this.centerY + Math.sin(this.angle) * (this.radius + this.length);
                context.strokeStyle = this.color;
                context.lineWidth = 4;
                context.beginPath();
                context.moveTo(x1, y1);
                context.lineTo(x2, y2);
                context.stroke();
            }
        }

        const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        const analyser = audioCtx.createAnalyser();
        analyser.fftSize = 64; // You can adjust for more/less bars
        const bufferLength = analyser.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);

        const source = audioCtx.createMediaElementSource(audio);
        source.connect(analyser);
        analyser.connect(audioCtx.destination);

        // Create an array to hold your Bar objects
        const bars = [];
        const centerX = canvas.width / 2;
        const centerY = canvas.height / 2;
        const radius = 40;
        for (let i = 0; i < bufferLength; i++) {
            const angle = (i / bufferLength) * Math.PI * 2;
            const color = `hsl(${200 + (i * 2) % 60}, 100%, 60%)`; // blue/cyan spiral
            bars.push(new Bar(centerX, centerY, radius, angle, color, i));
        }

        function animate() {
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            analyser.getByteFrequencyData(dataArray);

            for (let i = 0; i < bufferLength; i++) {
                bars[i].update(dataArray[i]);
                bars[i].draw(ctx);
            }
            requestAnimationFrame(animate);
        }
        audio.onplay = () => {
            if (audioCtx.state === 'suspended') audioCtx.resume();
            animate();
        };
        animate(); 
    </script>
</body>
</html>